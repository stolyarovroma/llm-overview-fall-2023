# Оптимизация инференса

## Введение

Большие языковые модели очень популярны в наши дни, они стали state-of-the-art подходом для многих задач. Однако данные модели очень требовательны к ресурсам во время обучения и использования. Чрезвычайно высокие затраты как по памяти, так и по времени во время инференса, является серьезным препятствием для внедрения трансформеров для решения реальных задач.

Почему инференс больших языковых моделей настолько ресурсозатратен? Помимо увеличения размера моделей SoTA, есть два основных фактора [Pope et al., 2022](https://arxiv.org/abs/2211.05102):

- *Большой расход памяти*. Как параметры модели, так и промежуточные состояния необходимы в памяти во время инференса. Например,
  - Кэш KV должен храниться в памяти во время стадии декодирования; например, для размера батча 512 и длины контекста 2048 общий объем кэша KV составляет 3 ТБ, что в 3 раза превышает размер модели (!).
  - Стоимость инференса из-за механизма attention квадратично зависит от длины входной последовательности.
- *Низкая распараллеливаемость*. Генерация выполняется авторегрессионным способом, что затрудняет распараллеливание процесса декодирования.

## Обзор методов

Цели оптимизации инференса модели:

- Сокращение объема памяти модели за счет использования меньшего количества устройств с GPU и меньшего объема памяти GPU;
- Снижение сложности вычислений за счет уменьшения FLOPs;
- Уменьшение inference latency и ускорение работы.

Можно использовать несколько методов, чтобы сделать инференс менее затратным по памяти и/или быстрее по времени.

- Применяйте параллелизм для масштабирования модели на большом количестве GPU. Умный параллелизм компонентов модели и данных позволяет запускать модель с триллионами параметров.
- Выгрузки временно неиспользуемых данных в CPU и последующего считывания их обратно при необходимости. Это помогает уменьшить использование памяти, но приводит к увеличению задержки.
- Особые стратегии пакетной обработки; Например, [EffectiveTransformer](https://github.com/bytedance/effective_transformer) объединяет последовательности входных данных, для того чтобы удалять паддинги в рамках одного батча.
- Методы сжатия сети, такие как прунинг, квантизация, дистилляция. Модель меньшего размера, с точки зрения количества параметров или разрядности, должна требовать меньше памяти и работать быстрее.
- Улучшения, характерные для архитектуры целевой модели. Многие архитектурные изменения, особенно для слоёв внимания, помогают повысить скорость стадии декодирования трансформера.

## Инструменты для разработчиков

**TensorRT-LLM** предоставляет пользователям простой в использовании Python API для описания больших языковых моделей (LLM) и создания движков TensorRT, которые содержат самые современные оптимизации для эффективного выполнения инференса на графических процессорах NVIDIA. TensorRT-LLM также содержит компоненты для создания сред выполнения на Python и C++, которые выполняют эти движки TensorRT.

![Alt text](tensorrt.png)


## Квантизация

Один из подходов - понижение битности вычислений. Если обычно веса и активации представлены простыми вещественными числами, то при переходе к int8 мы уменьшаем затрачиваемую память и сложность вычислений.


Итак, у нас есть значения с плавающей точкой, мы хотим перевести их в целочисленные значения в некотором диапазоне.

Как мы это будем делать?

Мы рассмотрим два вида квантизации: 

1) Асимметричная квантизация

Для этого нам нужны два параметра - scale (Δ) для масштаба и zero-point (z) для сдвига. Тогда квантизованное значение x<sub>Q</sub>  получается следующим образом:

x<sub>int</sub> = round(x / Δ) + z

x<sub>Q</sub> = clamp(0, N<sub>levels</sub> - 1, x<sub>int</sub>), (то есть ограничиваем снизу 0, а сверху N<sub>levels</sub> - 1)

Деквантизация происходит очевидным образом:

x<sub>float</sub>  = (x<sub>Q</sub> - z)Δ


2) Симметричная квантизация

В этом случае сдвига не происходит:

x<sub>int</sub> = round(x / Δ)

x<sub>Q</sub> = clamp(- Nlevels/2, Nlevels/2 - 1, xint), если нужна квантизация с разными знаками

x<sub>Q</sub> = clamp(0, Nlevels - 1, xint), иначе

Деквантизация в этом случае выглядит проще

x<sub>float</sub>  = x<sub>Q</sub> Δ

Плюс симметричной квантизации в том, что она сохраняет 0 на месте (очевидно по построению). Это выгодно для паддинга (поскольку происходит заполнение 0). Однако, благодаря сдвигу, асимметричная квантизация позволяет лучше распределить значения на интервале, а потому добиться лучшей точности.


Встаёт вопрос, как нам подбирать параметры. При квантизации часть информации неизбежно теряется, поэтому мы хотим использовать весь диапазон доступных значений.


Для активаций мы можем выбрать минимум и максимум от входов (или некоторую их квантиль)

Для весов мы действуем аналогично. Мы можем взять квантили по всему слою или по батчам, в таком случае у нас получится более точные предсказания.

Также обратим внимание, что при квантизации матожидание выхода у нас изменится. 

E[y<sub>Q</sub>] =E[W<sub>Q</sub>x] = E[(W + ΔW)x] = E[Wx] + E[ΔWx] 

Чтобы это исправить, мы можем посчитать эту ошибку на нескольких батчах и вычитать на выходе.

E[y<sub>corr</sub>] =E[W<sub>Q</sub>x] - E[ΔWx]   

Таким образом, пайплайн выглядит следующим образом:

1) Обучение полновесной модели

2) Калибровка параметров квантизации

3) Инференс


Встаёт вопрос, можем ли мы ещё как-то улучшить квантизацию на компьютере

Для этого существует QAT (Quantization-Aware Training), где мы сначала обучаем модель и проводим PTQ(описанную выше), а затем передаём параметры квантизации и обучаем модель ещё несколько эпох.

Заметим, что при обратном проходе производная почти везде будет равна 0.

В квантизации используют такую хитрость: при обратном проходе мы считаем, что функция у нас линейная и производная берётся именно от этой линейной функции.
